{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866dd5a6",
   "metadata": {},
   "source": [
    "## Testing Exceptions\n",
    "\n",
    "Often you need to test that your code **raises** exceptions for invalid inputs.\n",
    "\n",
    "### The Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(a, b):\n",
    "    \"\"\"Divide a by b.\"\"\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero\")\n",
    "    if not isinstance(a, (int, float)) or not isinstance(b, (int, float)):\n",
    "        raise TypeError(\"Both arguments must be numbers\")\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1acfc75",
   "metadata": {},
   "source": [
    "How do we test that these exceptions are raised correctly?\n",
    "\n",
    "### Using `pytest.raises()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ecae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "def test_divide_by_zero():\n",
    "    \"\"\"Test that dividing by zero raises ValueError.\"\"\"\n",
    "    with pytest.raises(ValueError):\n",
    "        divide(10, 0)\n",
    "\n",
    "def test_divide_invalid_type():\n",
    "    \"\"\"Test that invalid types raise TypeError.\"\"\"\n",
    "    with pytest.raises(TypeError):\n",
    "        divide(\"10\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dafddf4",
   "metadata": {},
   "source": [
    "**How it works:**\n",
    "- Code inside the `with` block is expected to raise an exception\n",
    "- ‚úÖ If the exception is raised ‚Üí test passes\n",
    "- ‚ùå If no exception is raised ‚Üí test fails\n",
    "- ‚ùå If a different exception is raised ‚Üí test fails\n",
    "\n",
    "### Checking Exception Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ccb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_divide_by_zero_with_message():\n",
    "    \"\"\"Test exception is raised with correct message.\"\"\"\n",
    "    with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n",
    "        divide(10, 0)\n",
    "\n",
    "def test_divide_invalid_type_with_message():\n",
    "    \"\"\"Test TypeError message.\"\"\"\n",
    "    with pytest.raises(TypeError, match=\"Both arguments must be numbers\"):\n",
    "        divide(\"text\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d88b1",
   "metadata": {},
   "source": [
    "The `match` parameter accepts a regex pattern. Test passes only if:\n",
    "1. Correct exception type is raised\n",
    "2. Exception message matches the pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073aee4",
   "metadata": {},
   "source": [
    "### Practical Example: User Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationError(Exception):\n",
    "    \"\"\"Custom validation error.\"\"\"\n",
    "    pass\n",
    "\n",
    "class User:\n",
    "    def __init__(self, username, email, age):\n",
    "        if not username:\n",
    "            raise ValidationError(\"Username cannot be empty\")\n",
    "        if \"@\" not in email:\n",
    "            raise ValidationError(\"Invalid email format\")\n",
    "        if age < 0:\n",
    "            raise ValidationError(\"Age cannot be negative\")\n",
    "        if age > 150:\n",
    "            raise ValidationError(\"Age is unrealistic\")\n",
    "        \n",
    "        self.username = username\n",
    "        self.email = email\n",
    "        self.age = age\n",
    "\n",
    "def test_user_empty_username():\n",
    "    \"\"\"Test empty username raises error.\"\"\"\n",
    "    with pytest.raises(ValidationError, match=\"Username cannot be empty\"):\n",
    "        User(\"\", \"alice@example.com\", 25)\n",
    "\n",
    "def test_user_invalid_email():\n",
    "    \"\"\"Test invalid email raises error.\"\"\"\n",
    "    with pytest.raises(ValidationError, match=\"Invalid email format\"):\n",
    "        User(\"alice\", \"invalid-email\", 25)\n",
    "\n",
    "def test_user_negative_age():\n",
    "    \"\"\"Test negative age raises error.\"\"\"\n",
    "    with pytest.raises(ValidationError, match=\"Age cannot be negative\"):\n",
    "        User(\"alice\", \"alice@example.com\", -5)\n",
    "\n",
    "def test_user_unrealistic_age():\n",
    "    \"\"\"Test unrealistic age raises error.\"\"\"\n",
    "    with pytest.raises(ValidationError, match=\"Age is unrealistic\"):\n",
    "        User(\"alice\", \"alice@example.com\", 200)\n",
    "\n",
    "def test_user_valid():\n",
    "    \"\"\"Test valid user creation doesn't raise error.\"\"\"\n",
    "    user = User(\"alice\", \"alice@example.com\", 25)\n",
    "    assert user.username == \"alice\"\n",
    "    assert user.email == \"alice@example.com\"\n",
    "    assert user.age == 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2182e348",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test Markers\n",
    "\n",
    "**Markers** are tags you can apply to tests to categorize them or control their behavior.\n",
    "\n",
    "### Built-in Markers\n",
    "\n",
    "#### `@pytest.mark.skip` - Skip a Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f403941",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.skip(reason=\"Feature not implemented yet\")\n",
    "def test_future_feature():\n",
    "    \"\"\"This test will be skipped.\"\"\"\n",
    "    # Will implement this later\n",
    "    pass\n",
    "\n",
    "@pytest.mark.skip(reason=\"Waiting for API endpoint\")\n",
    "def test_api_integration():\n",
    "    \"\"\"Skip until API is ready.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be60315",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "```\n",
    "test_example.py::test_future_feature SKIPPED (Feature not implemented yet)\n",
    "test_example.py::test_api_integration SKIPPED (Waiting for API endpoint)\n",
    "```\n",
    "\n",
    "Tests are skipped and the reason is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1cc34",
   "metadata": {},
   "source": [
    "#### `@pytest.mark.skipif` - Conditional Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ff493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "@pytest.mark.skipif(sys.platform == \"win32\", reason=\"Not supported on Windows\")\n",
    "def test_unix_specific():\n",
    "    \"\"\"This test only runs on Unix systems.\"\"\"\n",
    "    # Unix-specific code\n",
    "    pass\n",
    "\n",
    "@pytest.mark.skipif(sys.version_info < (3, 8), reason=\"Requires Python 3.8+\")\n",
    "def test_modern_python_feature():\n",
    "    \"\"\"This test requires Python 3.8 or higher.\"\"\"\n",
    "    # Use walrus operator := (Python 3.8+)\n",
    "    if (n := 10) > 5:\n",
    "        assert n == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061dbe34",
   "metadata": {},
   "source": [
    "**Common use cases:**\n",
    "- Platform-specific tests (Windows vs Linux vs Mac)\n",
    "- Python version requirements\n",
    "- Optional dependencies\n",
    "- Environment-specific tests (development vs production)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7e6c4",
   "metadata": {},
   "source": [
    "#### `@pytest.mark.xfail` - Expected to Fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e07d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.xfail(reason=\"Known bug #123\")\n",
    "def test_known_bug():\n",
    "    \"\"\"This test is expected to fail until bug #123 is fixed.\"\"\"\n",
    "    result = buggy_function()\n",
    "    assert result == expected_value\n",
    "\n",
    "@pytest.mark.xfail(sys.platform == \"win32\", reason=\"Windows bug\")\n",
    "def test_with_windows_bug():\n",
    "    \"\"\"Expected to fail on Windows.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8ac40",
   "metadata": {},
   "source": [
    "**Output possibilities:**\n",
    "- `XFAIL` - Test failed as expected (good!)\n",
    "- `XPASS` - Test passed unexpectedly (bug might be fixed!)\n",
    "\n",
    "**Use xfail when:**\n",
    "- You know a test will fail but want to track it\n",
    "- There's a known bug that will be fixed later\n",
    "- You're documenting expected failures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97324abd",
   "metadata": {},
   "source": [
    "**Development workflow:**\n",
    "```bash\n",
    "# Quick feedback during development\n",
    "pytest -m \"unit and fast\"  # Run only fast unit tests\n",
    "\n",
    "# Before committing\n",
    "pytest -m \"unit or smoke\"  # Unit tests + smoke tests\n",
    "\n",
    "# Full test suite\n",
    "pytest  # Run everything\n",
    "\n",
    "# CI/CD pipeline\n",
    "pytest -m \"not slow\"  # Skip slow tests for faster feedback\n",
    "pytest -m integration  # Run integration tests separately\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e28e7",
   "metadata": {},
   "source": [
    "### Running the Test Suite\n",
    "\n",
    "**Run all tests:**\n",
    "```bash\n",
    "pytest\n",
    "```\n",
    "\n",
    "**Run only unit tests (fast):**\n",
    "```bash\n",
    "pytest -m unit\n",
    "```\n",
    "\n",
    "**Run only integration tests:**\n",
    "```bash\n",
    "pytest -m integration\n",
    "```\n",
    "\n",
    "**Run with verbose output:**\n",
    "```bash\n",
    "pytest -v\n",
    "```\n",
    "\n",
    "**Run specific test file:**\n",
    "```bash\n",
    "pytest tests/test_user.py\n",
    "```\n",
    "\n",
    "**Run specific test class:**\n",
    "```bash\n",
    "pytest tests/test_user.py::TestUserValidation\n",
    "```\n",
    "\n",
    "**Run specific test:**\n",
    "```bash\n",
    "pytest tests/test_user.py::TestUserValidation::test_valid_user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee6e4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### 1. Organize Tests with Markers\n",
    "\n",
    "**‚úÖ Good - Well organized:**\n",
    "```python\n",
    "@pytest.mark.unit\n",
    "@pytest.mark.fast\n",
    "def test_addition():\n",
    "    ...\n",
    "\n",
    "@pytest.mark.integration\n",
    "@pytest.mark.slow\n",
    "@pytest.mark.database\n",
    "def test_full_workflow():\n",
    "    ...\n",
    "```\n",
    "\n",
    "### 2. Test Exceptions Explicitly\n",
    "\n",
    "**‚úÖ Good:**\n",
    "```python\n",
    "def test_division_by_zero():\n",
    "    with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n",
    "        divide(10, 0)\n",
    "```\n",
    "\n",
    "### 3. Use Descriptive Marker Names\n",
    "\n",
    "**‚ùå Bad:**\n",
    "```python\n",
    "@pytest.mark.test1\n",
    "@pytest.mark.slow\n",
    "```\n",
    "\n",
    "**‚úÖ Good:**\n",
    "```python\n",
    "@pytest.mark.integration\n",
    "@pytest.mark.database\n",
    "@pytest.mark.slow\n",
    "```\n",
    "\n",
    "### 4. Skip vs XFail\n",
    "\n",
    "**Use `skip` when:**\n",
    "- Test isn't relevant (platform-specific)\n",
    "- Feature not implemented yet\n",
    "- Missing dependencies\n",
    "\n",
    "**Use `xfail` when:**\n",
    "- Known bug that will be fixed\n",
    "- Test documents expected failure\n",
    "- Want to track failures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b242379",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "‚úÖ **pytest.raises()** - Test that exceptions are raised correctly  \n",
    "‚úÖ **Markers** - Tag and organize tests  \n",
    "‚úÖ **@pytest.mark.skip** - Skip tests  \n",
    "‚úÖ **@pytest.mark.skipif** - Conditional skip  \n",
    "‚úÖ **@pytest.mark.xfail** - Expected failures  \n",
    "‚úÖ **Custom markers** - Create your own test categories  \n",
    "‚úÖ **conftest.py** - Share fixtures across test files  \n",
    "‚úÖ **Running specific tests** - Use `-m` flag with markers  \n",
    "\n",
    "### Key Commands:\n",
    "\n",
    "```bash\n",
    "pytest -m unit                    # Run unit tests\n",
    "pytest -m \"not slow\"             # Skip slow tests\n",
    "pytest -m \"integration and database\"  # Multiple markers\n",
    "pytest -v                        # Verbose output\n",
    "pytest -k \"pattern\"              # Run tests matching pattern\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "You now have all the essential pytest knowledge! Practice by:\n",
    "- Writing tests for your own projects\n",
    "- Organizing tests with markers\n",
    "- Creating shared fixtures in conftest.py\n",
    "- Testing edge cases and exceptions\n",
    "\n",
    "Happy testing! üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
